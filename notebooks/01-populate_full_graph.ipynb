{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Let's now cut to the chase on Method 1\n",
    "\n",
    "We are going to skip ahead to the \"meat\" since the majority of the notebook before then is identical to `00`.  If you are running this on Google Colab, be sure to run the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /home/ubuntu/anaconda3/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "Requirement already satisfied: py2neo in /home/ubuntu/KnowledgeGraphs/venv/lib/python3.10/site-packages (2021.0.1)\n",
      "Requirement already satisfied: monotonic in /home/ubuntu/KnowledgeGraphs/venv/lib/python3.10/site-packages (from py2neo) (1.6)\n",
      "Requirement already satisfied: packaging in /home/ubuntu/KnowledgeGraphs/venv/lib/python3.10/site-packages (from py2neo) (23.1)\n",
      "Requirement already satisfied: pansi>=2020.7.3 in /home/ubuntu/KnowledgeGraphs/venv/lib/python3.10/site-packages (from py2neo) (2020.7.3)\n",
      "Requirement already satisfied: pytz in /home/ubuntu/KnowledgeGraphs/venv/lib/python3.10/site-packages (from py2neo) (2023.3)\n",
      "Requirement already satisfied: prompt-toolkit~=2.0.7 in /home/ubuntu/KnowledgeGraphs/venv/lib/python3.10/site-packages (from py2neo) (2.0.10)\n",
      "Requirement already satisfied: pygments>=2.0.0 in /home/ubuntu/KnowledgeGraphs/venv/lib/python3.10/site-packages (from py2neo) (2.16.1)\n",
      "Requirement already satisfied: neotime~=1.7.4 in /home/ubuntu/KnowledgeGraphs/venv/lib/python3.10/site-packages (from py2neo) (1.7.4)\n",
      "Requirement already satisfied: urllib3 in /home/ubuntu/KnowledgeGraphs/venv/lib/python3.10/site-packages (from py2neo) (2.0.4)\n",
      "Requirement already satisfied: certifi in /home/ubuntu/KnowledgeGraphs/venv/lib/python3.10/site-packages (from py2neo) (2023.7.22)\n",
      "Requirement already satisfied: english in /home/ubuntu/KnowledgeGraphs/venv/lib/python3.10/site-packages (from py2neo) (2020.7.0)\n",
      "Requirement already satisfied: docker in /home/ubuntu/KnowledgeGraphs/venv/lib/python3.10/site-packages (from py2neo) (6.1.3)\n",
      "Requirement already satisfied: six>=1.15.0 in /home/ubuntu/KnowledgeGraphs/venv/lib/python3.10/site-packages (from py2neo) (1.16.0)\n",
      "Requirement already satisfied: cryptography in /home/ubuntu/KnowledgeGraphs/venv/lib/python3.10/site-packages (from py2neo) (41.0.3)\n",
      "Requirement already satisfied: wcwidth in /home/ubuntu/KnowledgeGraphs/venv/lib/python3.10/site-packages (from prompt-toolkit~=2.0.7->py2neo) (0.2.6)\n",
      "Requirement already satisfied: cffi>=1.12 in /home/ubuntu/KnowledgeGraphs/venv/lib/python3.10/site-packages (from cryptography->py2neo) (1.15.1)\n",
      "Requirement already satisfied: websocket-client>=0.32.0 in /home/ubuntu/KnowledgeGraphs/venv/lib/python3.10/site-packages (from docker->py2neo) (1.6.2)\n",
      "Requirement already satisfied: requests>=2.26.0 in /home/ubuntu/KnowledgeGraphs/venv/lib/python3.10/site-packages (from docker->py2neo) (2.31.0)\n",
      "Requirement already satisfied: pycparser in /home/ubuntu/KnowledgeGraphs/venv/lib/python3.10/site-packages (from cffi>=1.12->cryptography->py2neo) (2.21)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ubuntu/KnowledgeGraphs/venv/lib/python3.10/site-packages (from requests>=2.26.0->docker->py2neo) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ubuntu/KnowledgeGraphs/venv/lib/python3.10/site-packages (from requests>=2.26.0->docker->py2neo) (3.4)\n",
      "/bin/bash: /home/ubuntu/anaconda3/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "Requirement already satisfied: wikipedia in /home/ubuntu/KnowledgeGraphs/venv/lib/python3.10/site-packages (1.4.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/ubuntu/KnowledgeGraphs/venv/lib/python3.10/site-packages (from wikipedia) (4.12.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /home/ubuntu/KnowledgeGraphs/venv/lib/python3.10/site-packages (from wikipedia) (2.31.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ubuntu/KnowledgeGraphs/venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ubuntu/KnowledgeGraphs/venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/KnowledgeGraphs/venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2023.7.22)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ubuntu/KnowledgeGraphs/venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.0.4)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/ubuntu/KnowledgeGraphs/venv/lib/python3.10/site-packages (from beautifulsoup4->wikipedia) (2.5)\n",
      "/bin/bash: /home/ubuntu/anaconda3/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "Collecting spacy==3.0.3\n",
      "  Downloading spacy-3.0.3.tar.gz (7.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[164 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m Copied /tmp/pip-install-1i8o64iw/spacy_cfdb8e57285a4519a8880595cd88511c/setup.cfg -> /tmp/pip-install-1i8o64iw/spacy_cfdb8e57285a4519a8880595cd88511c/spacy/tests/package\n",
      "  \u001b[31m   \u001b[0m Copied /tmp/pip-install-1i8o64iw/spacy_cfdb8e57285a4519a8880595cd88511c/pyproject.toml -> /tmp/pip-install-1i8o64iw/spacy_cfdb8e57285a4519a8880595cd88511c/spacy/tests/package\n",
      "  \u001b[31m   \u001b[0m Cythonizing sources\n",
      "  \u001b[31m   \u001b[0m Compiling spacy/training/example.pyx because it changed.\n",
      "  \u001b[31m   \u001b[0m Compiling spacy/parts_of_speech.pyx because it changed.\n",
      "  \u001b[31m   \u001b[0m Compiling spacy/strings.pyx because it changed.\n",
      "  \u001b[31m   \u001b[0m Compiling spacy/lexeme.pyx because it changed.\n",
      "  \u001b[31m   \u001b[0m Compiling spacy/vocab.pyx because it changed.\n",
      "  \u001b[31m   \u001b[0m Compiling spacy/attrs.pyx because it changed.\n",
      "  \u001b[31m   \u001b[0m Compiling spacy/kb.pyx because it changed.\n",
      "  \u001b[31m   \u001b[0m Compiling spacy/ml/parser_model.pyx because it changed.\n",
      "  \u001b[31m   \u001b[0m Compiling spacy/morphology.pyx because it changed.\n",
      "  \u001b[31m   \u001b[0m Compiling spacy/pipeline/dep_parser.pyx because it changed.\n",
      "  \u001b[31m   \u001b[0m Compiling spacy/pipeline/morphologizer.pyx because it changed.\n",
      "  \u001b[31m   \u001b[0m Compiling spacy/pipeline/multitask.pyx because it changed.\n",
      "  \u001b[31m   \u001b[0m Compiling spacy/pipeline/ner.pyx because it changed.\n",
      "  \u001b[31m   \u001b[0m Compiling spacy/pipeline/pipe.pyx because it changed.\n",
      "  \u001b[31m   \u001b[0m Compiling spacy/pipeline/trainable_pipe.pyx because it changed.\n",
      "  \u001b[31m   \u001b[0m Compiling spacy/pipeline/sentencizer.pyx because it changed.\n",
      "  \u001b[31m   \u001b[0m Compiling spacy/pipeline/senter.pyx because it changed.\n",
      "  \u001b[31m   \u001b[0m Compiling spacy/pipeline/tagger.pyx because it changed.\n",
      "  \u001b[31m   \u001b[0m Compiling spacy/pipeline/transition_parser.pyx because it changed.\n",
      "  \u001b[31m   \u001b[0m Compiling spacy/pipeline/_parser_internals/arc_eager.pyx because it changed.\n",
      "  \u001b[31m   \u001b[0m Compiling spacy/pipeline/_parser_internals/ner.pyx because it changed.\n",
      "  \u001b[31m   \u001b[0m Compiling spacy/pipeline/_parser_internals/nonproj.pyx because it changed.\n",
      "  \u001b[31m   \u001b[0m Compiling spacy/pipeline/_parser_internals/_state.pyx because it changed.\n",
      "  \u001b[31m   \u001b[0m Compiling spacy/pipeline/_parser_internals/stateclass.pyx because it changed.\n",
      "  \u001b[31m   \u001b[0m Compiling spacy/pipeline/_parser_internals/transition_system.pyx because it changed.\n",
      "  \u001b[31m   \u001b[0m Compiling spacy/pipeline/_parser_internals/_beam_utils.pyx because it changed.\n",
      "  \u001b[31m   \u001b[0m Compiling spacy/tokenizer.pyx because it changed.\n",
      "  \u001b[31m   \u001b[0m Compiling spacy/training/align.pyx because it changed.\n",
      "  \u001b[31m   \u001b[0m Compiling spacy/training/gold_io.pyx because it changed.\n",
      "  \u001b[31m   \u001b[0m Compiling spacy/tokens/doc.pyx because it changed.\n",
      "  \u001b[31m   \u001b[0m Compiling spacy/tokens/span.pyx because it changed.\n",
      "  \u001b[31m   \u001b[0m Compiling spacy/tokens/token.pyx because it changed.\n",
      "  \u001b[31m   \u001b[0m Compiling spacy/tokens/span_group.pyx because it changed.\n",
      "  \u001b[31m   \u001b[0m Compiling spacy/tokens/graph.pyx because it changed.\n",
      "  \u001b[31m   \u001b[0m Compiling spacy/tokens/morphanalysis.pyx because it changed.\n",
      "  \u001b[31m   \u001b[0m Compiling spacy/tokens/_retokenize.pyx because it changed.\n",
      "  \u001b[31m   \u001b[0m Compiling spacy/matcher/matcher.pyx because it changed.\n",
      "  \u001b[31m   \u001b[0m Compiling spacy/matcher/phrasematcher.pyx because it changed.\n",
      "  \u001b[31m   \u001b[0m Compiling spacy/matcher/dependencymatcher.pyx because it changed.\n",
      "  \u001b[31m   \u001b[0m Compiling spacy/symbols.pyx because it changed.\n",
      "  \u001b[31m   \u001b[0m Compiling spacy/vectors.pyx because it changed.\n",
      "  \u001b[31m   \u001b[0m [ 1/41] Cythonizing spacy/attrs.pyx\n",
      "  \u001b[31m   \u001b[0m [ 2/41] Cythonizing spacy/kb.pyx\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m Error compiling Cython file:\n",
      "  \u001b[31m   \u001b[0m ------------------------------------------------------------\n",
      "  \u001b[31m   \u001b[0m ...\n",
      "  \u001b[31m   \u001b[0m     int length\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m cdef class Vocab:\n",
      "  \u001b[31m   \u001b[0m     cdef Pool mem\n",
      "  \u001b[31m   \u001b[0m     cpdef readonly StringStore strings\n",
      "  \u001b[31m   \u001b[0m           ^\n",
      "  \u001b[31m   \u001b[0m ------------------------------------------------------------\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m spacy/vocab.pxd:28:10: Variables cannot be declared with 'cpdef'. Use 'cdef' instead.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m Error compiling Cython file:\n",
      "  \u001b[31m   \u001b[0m ------------------------------------------------------------\n",
      "  \u001b[31m   \u001b[0m ...\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m cdef class Vocab:\n",
      "  \u001b[31m   \u001b[0m     cdef Pool mem\n",
      "  \u001b[31m   \u001b[0m     cpdef readonly StringStore strings\n",
      "  \u001b[31m   \u001b[0m     cpdef public Morphology morphology\n",
      "  \u001b[31m   \u001b[0m           ^\n",
      "  \u001b[31m   \u001b[0m ------------------------------------------------------------\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m spacy/vocab.pxd:29:10: Variables cannot be declared with 'cpdef'. Use 'cdef' instead.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m Error compiling Cython file:\n",
      "  \u001b[31m   \u001b[0m ------------------------------------------------------------\n",
      "  \u001b[31m   \u001b[0m ...\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m cdef class Vocab:\n",
      "  \u001b[31m   \u001b[0m     cdef Pool mem\n",
      "  \u001b[31m   \u001b[0m     cpdef readonly StringStore strings\n",
      "  \u001b[31m   \u001b[0m     cpdef public Morphology morphology\n",
      "  \u001b[31m   \u001b[0m     cpdef public object vectors\n",
      "  \u001b[31m   \u001b[0m           ^\n",
      "  \u001b[31m   \u001b[0m ------------------------------------------------------------\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m spacy/vocab.pxd:30:10: Variables cannot be declared with 'cpdef'. Use 'cdef' instead.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m Error compiling Cython file:\n",
      "  \u001b[31m   \u001b[0m ------------------------------------------------------------\n",
      "  \u001b[31m   \u001b[0m ...\n",
      "  \u001b[31m   \u001b[0m cdef class Vocab:\n",
      "  \u001b[31m   \u001b[0m     cdef Pool mem\n",
      "  \u001b[31m   \u001b[0m     cpdef readonly StringStore strings\n",
      "  \u001b[31m   \u001b[0m     cpdef public Morphology morphology\n",
      "  \u001b[31m   \u001b[0m     cpdef public object vectors\n",
      "  \u001b[31m   \u001b[0m     cpdef public object _lookups\n",
      "  \u001b[31m   \u001b[0m           ^\n",
      "  \u001b[31m   \u001b[0m ------------------------------------------------------------\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m spacy/vocab.pxd:31:10: Variables cannot be declared with 'cpdef'. Use 'cdef' instead.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m Error compiling Cython file:\n",
      "  \u001b[31m   \u001b[0m ------------------------------------------------------------\n",
      "  \u001b[31m   \u001b[0m ...\n",
      "  \u001b[31m   \u001b[0m     cdef Pool mem\n",
      "  \u001b[31m   \u001b[0m     cpdef readonly StringStore strings\n",
      "  \u001b[31m   \u001b[0m     cpdef public Morphology morphology\n",
      "  \u001b[31m   \u001b[0m     cpdef public object vectors\n",
      "  \u001b[31m   \u001b[0m     cpdef public object _lookups\n",
      "  \u001b[31m   \u001b[0m     cpdef public object writing_system\n",
      "  \u001b[31m   \u001b[0m           ^\n",
      "  \u001b[31m   \u001b[0m ------------------------------------------------------------\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m spacy/vocab.pxd:32:10: Variables cannot be declared with 'cpdef'. Use 'cdef' instead.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m Error compiling Cython file:\n",
      "  \u001b[31m   \u001b[0m ------------------------------------------------------------\n",
      "  \u001b[31m   \u001b[0m ...\n",
      "  \u001b[31m   \u001b[0m     cpdef readonly StringStore strings\n",
      "  \u001b[31m   \u001b[0m     cpdef public Morphology morphology\n",
      "  \u001b[31m   \u001b[0m     cpdef public object vectors\n",
      "  \u001b[31m   \u001b[0m     cpdef public object _lookups\n",
      "  \u001b[31m   \u001b[0m     cpdef public object writing_system\n",
      "  \u001b[31m   \u001b[0m     cpdef public object get_noun_chunks\n",
      "  \u001b[31m   \u001b[0m           ^\n",
      "  \u001b[31m   \u001b[0m ------------------------------------------------------------\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m spacy/vocab.pxd:33:10: Variables cannot be declared with 'cpdef'. Use 'cdef' instead.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m Error compiling Cython file:\n",
      "  \u001b[31m   \u001b[0m ------------------------------------------------------------\n",
      "  \u001b[31m   \u001b[0m ...\n",
      "  \u001b[31m   \u001b[0m     cdef float prior_prob\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m cdef class KnowledgeBase:\n",
      "  \u001b[31m   \u001b[0m     cdef Pool mem\n",
      "  \u001b[31m   \u001b[0m     cpdef readonly Vocab vocab\n",
      "  \u001b[31m   \u001b[0m           ^\n",
      "  \u001b[31m   \u001b[0m ------------------------------------------------------------\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m spacy/kb.pxd:31:10: Variables cannot be declared with 'cpdef'. Use 'cdef' instead.\n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"/home/ubuntu/KnowledgeGraphs/venv/lib/python3.10/site-packages/pip/_vendor/pep517/in_process/_in_process.py\", line 363, in <module>\n",
      "  \u001b[31m   \u001b[0m     main()\n",
      "  \u001b[31m   \u001b[0m   File \"/home/ubuntu/KnowledgeGraphs/venv/lib/python3.10/site-packages/pip/_vendor/pep517/in_process/_in_process.py\", line 345, in main\n",
      "  \u001b[31m   \u001b[0m     json_out['return_val'] = hook(**hook_input['kwargs'])\n",
      "  \u001b[31m   \u001b[0m   File \"/home/ubuntu/KnowledgeGraphs/venv/lib/python3.10/site-packages/pip/_vendor/pep517/in_process/_in_process.py\", line 130, in get_requires_for_build_wheel\n",
      "  \u001b[31m   \u001b[0m     return hook(config_settings)\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-i94pfnyp/overlay/lib/python3.10/site-packages/setuptools/build_meta.py\", line 355, in get_requires_for_build_wheel\n",
      "  \u001b[31m   \u001b[0m     return self._get_build_requires(config_settings, requirements=['wheel'])\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-i94pfnyp/overlay/lib/python3.10/site-packages/setuptools/build_meta.py\", line 325, in _get_build_requires\n",
      "  \u001b[31m   \u001b[0m     self.run_setup()\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-i94pfnyp/overlay/lib/python3.10/site-packages/setuptools/build_meta.py\", line 341, in run_setup\n",
      "  \u001b[31m   \u001b[0m     exec(code, locals())\n",
      "  \u001b[31m   \u001b[0m   File \"<string>\", line 225, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"<string>\", line 211, in setup_package\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-i94pfnyp/overlay/lib/python3.10/site-packages/Cython/Build/Dependencies.py\", line 1134, in cythonize\n",
      "  \u001b[31m   \u001b[0m     cythonize_one(*args)\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-i94pfnyp/overlay/lib/python3.10/site-packages/Cython/Build/Dependencies.py\", line 1301, in cythonize_one\n",
      "  \u001b[31m   \u001b[0m     raise CompileError(None, pyx_file)\n",
      "  \u001b[31m   \u001b[0m Cython.Compiler.Errors.CompileError: spacy/kb.pyx\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[?25h\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
      "\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n"
     ]
    }
   ],
   "source": [
    "!pip install py2neo\n",
    "!pip install wikipedia\n",
    "!pip install spacy==3.0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.1\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import urllib\n",
    "from pprint import pprint\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "from py2neo import Node, Graph, Relationship, NodeMatcher\n",
    "from py2neo.bulk import merge_nodes\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wikipedia\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.tokens import Doc, Span, Token\n",
    "\n",
    "print(spacy.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "We are now going to create a slightly larger graph, this time including Michelle Obama (who should obviously be there, but didn't happen to be part of Barack Obama's Wikipedia summary).  \n",
    "\n",
    "I am going to skip some of the notebook comments from the previous notebook and just include all of the functions we used then at the top of this notebook.  I am also going to create a few helper functions (the last cell of functions before the fun begins) that will make the code a bit \"easier on the eyes.\"  So please feel free to skip on ahead to the bottom of the notebook, knowing that the cell here with all of the functions is the concatenation of all of those functions from the previous notebook.\n",
    "\n",
    "## If you are running this on Google Colab, don't forget to import the language model in the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /home/ubuntu/anaconda3/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "Collecting en-core-web-md==3.6.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.6.0/en_core_web_md-3.6.0-py3-none-any.whl (42.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /home/ubuntu/KnowledgeGraphs/venv/lib/python3.10/site-packages (from en-core-web-md==3.6.0) (3.6.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/ubuntu/KnowledgeGraphs/venv/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (2.4.7)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/ubuntu/KnowledgeGraphs/venv/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (2.0.7)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /home/ubuntu/KnowledgeGraphs/venv/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (2.3.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/ubuntu/KnowledgeGraphs/venv/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (1.25.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/ubuntu/KnowledgeGraphs/venv/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (1.0.9)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/ubuntu/KnowledgeGraphs/venv/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (1.1.2)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /home/ubuntu/KnowledgeGraphs/venv/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (8.1.12)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /home/ubuntu/KnowledgeGraphs/venv/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (0.10.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/ubuntu/KnowledgeGraphs/venv/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (3.0.12)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/ubuntu/KnowledgeGraphs/venv/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (4.66.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/ubuntu/KnowledgeGraphs/venv/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (2.31.0)\n",
      "Requirement already satisfied: jinja2 in /home/ubuntu/KnowledgeGraphs/venv/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (3.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ubuntu/KnowledgeGraphs/venv/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (23.1)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /home/ubuntu/KnowledgeGraphs/venv/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (0.9.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/ubuntu/KnowledgeGraphs/venv/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (3.0.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/ubuntu/KnowledgeGraphs/venv/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (2.0.9)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/ubuntu/KnowledgeGraphs/venv/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (3.3.0)\n",
      "Requirement already satisfied: setuptools in /home/ubuntu/KnowledgeGraphs/venv/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (59.6.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/ubuntu/KnowledgeGraphs/venv/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (1.0.4)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/ubuntu/KnowledgeGraphs/venv/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (6.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/ubuntu/KnowledgeGraphs/venv/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (0.5.0)\n",
      "Requirement already satisfied: pydantic-core==2.6.3 in /home/ubuntu/KnowledgeGraphs/venv/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (2.6.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /home/ubuntu/KnowledgeGraphs/venv/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (4.7.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ubuntu/KnowledgeGraphs/venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (2.0.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ubuntu/KnowledgeGraphs/venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ubuntu/KnowledgeGraphs/venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/KnowledgeGraphs/venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (2023.7.22)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/ubuntu/KnowledgeGraphs/venv/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (0.7.10)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/ubuntu/KnowledgeGraphs/venv/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (0.1.1)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/ubuntu/KnowledgeGraphs/venv/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (8.1.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ubuntu/KnowledgeGraphs/venv/lib/python3.10/site-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (2.1.3)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_md')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function spacy.pipeline.functions.merge_noun_chunks(doc: spacy.tokens.doc.Doc) -> spacy.tokens.doc.Doc>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SUBJECTS = [\"nsubj\", \"nsubjpass\", \"csubj\", \"csubjpass\", \"agent\", \"expl\"]\n",
    "VERBS = ['ROOT', 'advcl']\n",
    "OBJECTS = [\"dobj\", \"dative\", \"attr\", \"oprd\", 'pobj']\n",
    "ENTITY_LABELS = ['PERSON', 'NORP', 'GPE', 'ORG', 'FAC', 'LOC', 'PRODUCT', 'EVENT', 'WORK_OF_ART']\n",
    "\n",
    "api_key = open('.api_key').read()\n",
    "\n",
    "non_nc = spacy.load('en_core_web_md')\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "nlp.add_pipe('merge_noun_chunks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_google(query, api_key, limit=10, indent=True, return_lists=True):\n",
    "    \n",
    "    text_ls = []\n",
    "    node_label_ls = []\n",
    "    url_ls = []\n",
    "    \n",
    "    params = {\n",
    "        'query': query,\n",
    "        'limit': limit,\n",
    "        'indent': indent,\n",
    "        'key': api_key,\n",
    "    }   \n",
    "    \n",
    "    service_url = 'https://kgsearch.googleapis.com/v1/entities:search'\n",
    "    url = service_url + '?' + urllib.parse.urlencode(params)\n",
    "    response = json.loads(urllib.request.urlopen(url).read())\n",
    "    \n",
    "    if return_lists:\n",
    "        for element in response['itemListElement']:\n",
    "\n",
    "            try:\n",
    "                node_label_ls.append(element['result']['@type'])\n",
    "            except:\n",
    "                node_label_ls.append('')\n",
    "\n",
    "            try:\n",
    "                text_ls.append(element['result']['detailedDescription']['articleBody'])\n",
    "            except:\n",
    "                text_ls.append('')\n",
    "                \n",
    "            try:\n",
    "                url_ls.append(element['result']['detailedDescription']['url'])\n",
    "            except:\n",
    "                url_ls.append('')\n",
    "                \n",
    "        return text_ls, node_label_ls, url_ls\n",
    "    \n",
    "    else:\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_characters(text):\n",
    "    \n",
    "    regex = re.compile(r'[\\n\\r\\t]')\n",
    "    clean_text = regex.sub(\" \", text)\n",
    "    \n",
    "    return clean_text\n",
    "\n",
    "\n",
    "def remove_stop_words_and_punct(text, print_text=False):\n",
    "    \n",
    "    result_ls = []\n",
    "    rsw_doc = non_nc(text)\n",
    "    \n",
    "    for token in rsw_doc:\n",
    "        if print_text:\n",
    "            print(token, token.is_stop)\n",
    "            print('--------------')\n",
    "        if not token.is_stop and not token.is_punct:\n",
    "            result_ls.append(str(token))\n",
    "    \n",
    "    result_str = ' '.join(result_ls)\n",
    "\n",
    "    return result_str\n",
    "\n",
    "\n",
    "def create_svo_lists(doc, print_lists):\n",
    "    \n",
    "    subject_ls = []\n",
    "    verb_ls = []\n",
    "    object_ls = []\n",
    "\n",
    "    for token in doc:\n",
    "        if token.dep_ in SUBJECTS:\n",
    "            subject_ls.append((token.lower_, token.idx))\n",
    "        elif token.dep_ in VERBS:\n",
    "            verb_ls.append((token.lemma_, token.idx))\n",
    "        elif token.dep_ in OBJECTS:\n",
    "            object_ls.append((token.lower_, token.idx))\n",
    "\n",
    "    if print_lists:\n",
    "        print('SUBJECTS: ', subject_ls)\n",
    "        print('VERBS: ', verb_ls)\n",
    "        print('OBJECTS: ', object_ls)\n",
    "    \n",
    "    return subject_ls, verb_ls, object_ls\n",
    "\n",
    "\n",
    "def remove_duplicates(tup, tup_posn):\n",
    "    \n",
    "    check_val = set()\n",
    "    result = []\n",
    "    \n",
    "    for i in tup:\n",
    "        if i[tup_posn] not in check_val:\n",
    "            result.append(i)\n",
    "            check_val.add(i[tup_posn])\n",
    "            \n",
    "    return result\n",
    "\n",
    "\n",
    "def remove_dates(tup_ls):\n",
    "    \n",
    "    clean_tup_ls = []\n",
    "    for entry in tup_ls:\n",
    "        if not entry[2].isdigit():\n",
    "            clean_tup_ls.append(entry)\n",
    "    return clean_tup_ls\n",
    "\n",
    "\n",
    "def create_svo_triples(text, print_lists=False):\n",
    "    \n",
    "    clean_text = remove_special_characters(text)\n",
    "    doc = nlp(clean_text)\n",
    "    subject_ls, verb_ls, object_ls = create_svo_lists(doc, print_lists=print_lists)\n",
    "    \n",
    "    graph_tup_ls = []\n",
    "    dedup_tup_ls = []\n",
    "    clean_tup_ls = []\n",
    "    \n",
    "    for subj in subject_ls: \n",
    "        for obj in object_ls:\n",
    "            \n",
    "            dist_ls = []\n",
    "            \n",
    "            for v in verb_ls:\n",
    "                \n",
    "                # Assemble a list of distances between each object and each verb\n",
    "                dist_ls.append(abs(obj[1] - v[1]))\n",
    "                \n",
    "            # Get the index of the verb with the smallest distance to the object \n",
    "            # and return that verb\n",
    "            index_min = min(range(len(dist_ls)), key=dist_ls.__getitem__)\n",
    "            \n",
    "            # Remve stop words from subjects and object.  Note that we do this a bit\n",
    "            # later down in the process to allow for proper sentence recognition.\n",
    "\n",
    "            no_sw_subj = remove_stop_words_and_punct(subj[0])\n",
    "            no_sw_obj = remove_stop_words_and_punct(obj[0])\n",
    "            \n",
    "            # Add entries to the graph iff neither subject nor object is blank\n",
    "            if no_sw_subj and no_sw_obj:\n",
    "                tup = (no_sw_subj, verb_ls[index_min][0], no_sw_obj)\n",
    "                graph_tup_ls.append(tup)\n",
    "        \n",
    "        #clean_tup_ls = remove_dates(graph_tup_ls)\n",
    "    \n",
    "    dedup_tup_ls = remove_duplicates(graph_tup_ls, 2)\n",
    "    clean_tup_ls = remove_dates(dedup_tup_ls)\n",
    "    \n",
    "    return clean_tup_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_obj_properties(tup_ls):\n",
    "    \n",
    "    init_obj_tup_ls = []\n",
    "    \n",
    "    for tup in tup_ls:\n",
    "\n",
    "        try:\n",
    "            text, node_label_ls, url = query_google(tup[2], api_key, limit=1)\n",
    "            new_tup = (tup[0], tup[1], tup[2], text[0], node_label_ls[0], url[0])\n",
    "        except:\n",
    "            new_tup = (tup[0], tup[1], tup[2], [], [], [])\n",
    "        \n",
    "        init_obj_tup_ls.append(new_tup)\n",
    "        \n",
    "    return init_obj_tup_ls\n",
    "\n",
    "\n",
    "def add_layer(tup_ls):\n",
    "\n",
    "    svo_tup_ls = []\n",
    "    \n",
    "    for tup in tup_ls:\n",
    "        \n",
    "        if tup[3]:\n",
    "            svo_tup = create_svo_triples(tup[3])\n",
    "            svo_tup_ls.extend(svo_tup)\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    return get_obj_properties(svo_tup_ls)\n",
    "        \n",
    "\n",
    "def subj_equals_obj(tup_ls):\n",
    "    \n",
    "    new_tup_ls = []\n",
    "    \n",
    "    for tup in tup_ls:\n",
    "        if tup[0] != tup[2]:\n",
    "            new_tup_ls.append((tup[0], tup[1], tup[2], tup[3], tup[4], tup[5]))\n",
    "            \n",
    "    return new_tup_ls\n",
    "\n",
    "\n",
    "def check_for_string_labels(tup_ls):\n",
    "    # This is for an edge case where the object does not get fully populated\n",
    "    # resulting in the node labels being assigned to string instead of list.\n",
    "    # This may not be strictly necessary and the lines using it are commnted out\n",
    "    # below.  Run this function if you come across this case.\n",
    "    \n",
    "    clean_tup_ls = []\n",
    "    \n",
    "    for el in tup_ls:\n",
    "        if isinstance(el[2], list):\n",
    "            clean_tup_ls.append(el)\n",
    "            \n",
    "    return clean_tup_ls\n",
    "\n",
    "\n",
    "def create_word_vectors(tup_ls):\n",
    "\n",
    "    new_tup_ls = []\n",
    "    \n",
    "    for tup in tup_ls:\n",
    "        if tup[3]:\n",
    "            doc = nlp(tup[3])\n",
    "            new_tup = (tup[0], tup[1], tup[2], tup[3], tup[4], tup[5], doc.vector)\n",
    "        else:\n",
    "            new_tup = (tup[0], tup[1], tup[2], tup[3], tup[4], tup[5], np.random.uniform(low=-1.0, high=1.0, size=(300,)))\n",
    "        new_tup_ls.append(new_tup)\n",
    "        \n",
    "    return new_tup_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dedup(tup_ls):\n",
    "    \n",
    "    visited = set()\n",
    "    output_ls = []\n",
    "    \n",
    "    for tup in tup_ls:\n",
    "        if not tup[0] in visited:\n",
    "            visited.add(tup[0])\n",
    "            output_ls.append((tup[0], tup[1], tup[2], tup[3], tup[4]))\n",
    "            \n",
    "    return output_ls\n",
    "\n",
    "\n",
    "def convert_vec_to_ls(tup_ls):\n",
    "    \n",
    "    vec_to_ls_tup = []\n",
    "    \n",
    "    for el in tup_ls:\n",
    "        vec_ls = [float(v) for v in el[4]]\n",
    "        tup = (el[0], el[1], el[2], el[3], vec_ls)\n",
    "        vec_to_ls_tup.append(tup)\n",
    "        \n",
    "    return vec_to_ls_tup\n",
    "\n",
    "\n",
    "def add_nodes(tup_ls):   \n",
    "\n",
    "    keys = ['name', 'description', 'node_labels', 'url', 'word_vec']\n",
    "    merge_nodes(graph.auto(), tup_ls, ('Node', 'name'), keys=keys)\n",
    "    print('Number of nodes in graph: ', graph.nodes.match('Node').count())\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_edges(edge_ls):\n",
    "    \n",
    "    edge_dc = {} \n",
    "    \n",
    "    # Group tuple by verb\n",
    "    # Result: {verb1: [(sub1, v1, obj1), (sub2, v2, obj2), ...],\n",
    "    #          verb2: [(sub3, v3, obj3), (sub4, v4, obj4), ...]}\n",
    "    \n",
    "    for tup in edge_ls: \n",
    "        if tup[1] in edge_dc: \n",
    "            edge_dc[tup[1]].append((tup[0], tup[1], tup[2])) \n",
    "        else: \n",
    "            edge_dc[tup[1]] = [(tup[0], tup[1], tup[2])] \n",
    "    \n",
    "    for edge_labels, tup_ls in tqdm(edge_dc.items()):   # k=edge labels, v = list of tuples\n",
    "        \n",
    "        tx = graph.begin()\n",
    "        \n",
    "        for el in tup_ls:\n",
    "            source_node = nodes_matcher.match(name=el[0]).first()\n",
    "            target_node = nodes_matcher.match(name=el[2]).first()\n",
    "            if not source_node:\n",
    "                source_node = Node('Node', name=el[0])\n",
    "                tx.create(source_node)\n",
    "            if not target_node:\n",
    "                try:\n",
    "                    target_node = Node('Node', name=el[2], node_labels=el[4], url=el[5], word_vec=el[6])\n",
    "                    tx.create(target_node)\n",
    "                except:\n",
    "                    continue\n",
    "            try:\n",
    "                rel = Relationship(source_node, edge_labels, target_node)\n",
    "            except:\n",
    "                continue\n",
    "            tx.create(rel)\n",
    "        tx.commit()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Jump back in here!\n",
    "\n",
    "Now we are going to create the nodes and edges for Barack and Michelle Obama and populate the database with them.\n",
    "\n",
    "To do so, I have combined several of the steps in the previous notebook into a couple of helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def edge_tuple_creation(text):\n",
    "    \n",
    "    initial_tup_ls = create_svo_triples(text)\n",
    "    init_obj_tup_ls = get_obj_properties(initial_tup_ls)\n",
    "    new_layer_ls = add_layer(init_obj_tup_ls)\n",
    "    starter_edge_ls = init_obj_tup_ls + new_layer_ls\n",
    "    edge_ls = subj_equals_obj(starter_edge_ls)\n",
    "    edges_word_vec_ls = create_word_vectors(edge_ls)\n",
    "    \n",
    "    return edges_word_vec_ls\n",
    "\n",
    "\n",
    "def node_tuple_creation(edges_word_vec_ls):\n",
    "    \n",
    "    orig_node_tup_ls = [(edges_word_vec_ls[0][0], '', ['Subject'], '', np.random.uniform(low=-1.0, high=1.0, size=(300,)))]\n",
    "    obj_node_tup_ls = [(tup[2], tup[3], tup[4], tup[5], tup[6]) for tup in edges_word_vec_ls]\n",
    "    full_node_tup_ls = orig_node_tup_ls + obj_node_tup_ls\n",
    "    cleaned_node_tup_ls = check_for_string_labels(full_node_tup_ls)\n",
    "    #dedup_node_tup_ls = dedup(cleaned_node_tup_ls)\n",
    "    dedup_node_tup_ls = cleaned_node_tup_ls\n",
    "    node_tup_ls = convert_vec_to_ls(dedup_node_tup_ls)\n",
    "    \n",
    "    return node_tup_ls    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# If you are using a Sandbox instance, you will want to use the following (commented) line.  \n",
    "# If you are using a Docker container for your DB, use the uncommented line.\n",
    "# graph = Graph(\"bolt://some_ip_address:7687\", name=\"neo4j\", password=\"some_password\")\n",
    "# If you are using a Sandbox instance, you will want to use the following (commented) line.  \n",
    "# If you are using a Docker container for your DB, use the uncommented line.\n",
    "graph = Graph(\"bolt://34.238.38.220:7687\", name=\"neo4j\", password=\"attackers-resident-folders\")\n",
    "\n",
    "# graph = Graph(\"bolt://neo4j:7687\", name=\"neo4j\", password=\"kgDemo\")\n",
    "nodes_matcher = NodeMatcher(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 54.8 s, sys: 219 ms, total: 55.1 s\n",
      "Wall time: 2min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "barack_text = wikipedia.summary('barack obama')\n",
    "barack_edges_word_vec_ls = edge_tuple_creation(barack_text)\n",
    "barack_node_tup_ls = node_tuple_creation(barack_edges_word_vec_ls)\n",
    "\n",
    "michelle_text = wikipedia.summary('michelle obama')\n",
    "michelle_edges_word_vec_ls = edge_tuple_creation(michelle_text)\n",
    "michelle_node_tup_ls = node_tuple_creation(michelle_edges_word_vec_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "657 531\n"
     ]
    }
   ],
   "source": [
    "full_node_ls = barack_node_tup_ls + michelle_node_tup_ls\n",
    "full_edge_ls = barack_edges_word_vec_ls + michelle_edges_word_vec_ls\n",
    "full_dedup_node_tup_ls = dedup(full_node_ls)\n",
    "print(len(full_node_ls), len(full_dedup_node_tup_ls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes in graph:  1279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [00:28<00:00,  2.66it/s]\n"
     ]
    }
   ],
   "source": [
    "add_nodes(full_dedup_node_tup_ls)\n",
    "add_edges(full_edge_ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Next steps...\n",
    "\n",
    "We now have a graph that we can tinker with.  I would encourage you to explore the Cypher queries that can be found in `cypher_queries/`.  You will need to run several of these in order to use the next notebook, where we will explore doing some machine learning on the graph.  Note that any Cypher query can be done using `py2neo` or the official Neo4j python driver.  In the workshop, I will demonstrate these using the Neo4j browser, which you will find at `localhost:7474`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
